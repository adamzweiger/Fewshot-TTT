[
  {
    "task": "boolean_expressions",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 81.25,
    "ft_time": 12.807,
    "eval_time": 2.5711,
    "examples": [
      {
        "question": "( not False or True ) and True is",
        "prediction": "true",
        "true_answer": "true"
      },
      {
        "question": "not ( True and True ) or False is",
        "prediction": "false",
        "true_answer": "false"
      },
      {
        "question": "( False or True and not True ) is",
        "prediction": "true",
        "true_answer": "false"
      },
      {
        "question": "False and False and not True and False is",
        "prediction": "false",
        "true_answer": "false"
      },
      {
        "question": "not True and True and True or False is",
        "prediction": "false",
        "true_answer": "false"
      }
    ]
  },
  {
    "task": "causal_judgement",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 54.8023,
    "ft_time": 23.9568,
    "eval_time": 16.2807,
    "examples": [
      {
        "question": "How would a typical person answer each of the following questions about causation?\nA machine is set up in such a way that it will short circuit if both the black wire and the red wire touch the battery at the same time. The machine will not short circuit if just one of these wires touches the battery. The black wire is designated as the one that is supposed to touch the battery, while the red wire is supposed to remain in some other part of the machine. One day, the black wire and the red wire both end up touching the battery at the same time. There is a short circuit. Did the fact that the red wire touched the battery cause the machine to short circuit?\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "How would a typical person answer each of the following questions about causation?\nEugene and Tina were a young married couple who lived in the country. Both were partially paralyzed and confined to wheelchairs. They had met four years before when Tina was a counsellor with the Canadian Paraplegic Association, had fallen in love, and were married one year later. On this particular evening, Eugene had phoned to request a cab to take them downtown. When the taxi driver arrived, Eugene and Tina were waiting by the street. On seeing that they were both in wheelchairs, the taxi driver refused their fare because he thought it would be too crowded in the taxi with both of them and the wheelchairs. So the taxi driver headed back downtown without them. Because there was no time to call another cab, Eugene and Tina took Tina's car, which was equipped with special hand controls. In order to get downtown from their house, they had to travel across a bridge over Rupert River. A severe storm the night before had weakened the structure of the bridge. About 5 minutes before Eugene and Tina reached it, a section of the bridge collapsed. The taxi driver had reached the bridge about 15 minutes before them, and made it safely across. In the dark, Eugene and Tina drove off the collapsed bridge and their car plummeted into the river below. They both drowned. Their bodies were retrieved from the car the next morning. Did the taxi driver's refusal to take Eugene and Tina cause their death?\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "How would a typical person answer each of the following questions about causation?\nJoe was about to go pick up his child from a school in the suburbs. In the parking lot, he stopped to talk with friends. Because of this delay in picking up his son, Joe's neighbor, who waited 15 min for Joe to show up, took the child home in his car. On the way home, the neighbor's car was struck by a drunk driver. Joe's son, the only seriously hurt victim, received severe leg injuries. Did Joe cause his son's injury?\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "How would a typical person answer each of the following questions about causation?\nThe receptionist in the philosophy department keeps her desk stocked with pens. The administrative assistants are allowed to take the pens, but faculty members are supposed to buy their own. The administrative assistants typically do take the pens. Unfortunately, so do the faculty members. The receptionist has repeatedly emailed them reminders that only administrative assistants are allowed to take the pens. On Monday morning, one of the administrative assistants encounters Professor Smith walking past the receptionist's desk. Both take pens. Later that day, the receptionist needs to take an important message... but she has a problem. There are no pens left on her desk. Did Professor Smith cause the problem?\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "How would a typical person answer each of the following questions about causation?\nMark wants to detonate a bomb to kill some innocent people. He knows that only one button will detonate the bomb and that the other buttons will deactivate it. He thinks that the right button has the initials \"R.E.D.\" on it. But he is rushing, and he presses the red colored button instead. As it happens, the red colored button was the button for detonating the bomb. Mark detonates the bomb and kills some innocent people. Did Mark intentionally kill some people?\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      }
    ]
  },
  {
    "task": "date_understanding",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 51.25,
    "ft_time": 16.3324,
    "eval_time": 10.1945,
    "examples": [
      {
        "question": "Today is 9/7. Jane is watching NFL 2003. What is the date one year ago from today in MM/DD/YYYY?\nOptions:\n(A) 12/07/2002\n(B) 08/24/2002\n(C) 09/07/2002\n(D) 09/07/1918\n(E) 09/08/2002\n(F) 09/28/2002",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "Jane got her job in 2016. Today is her 3-year work anniversary. She still remember that on Dec 2, her second day at work, she spilled coffee on her laptop. What is the date one week ago from today in MM/DD/YYYY?\nOptions:\n(A) 11/23/2019\n(B) 06/24/2019\n(C) 03/24/2019\n(D) 11/24/2019\n(E) 02/29/2020",
        "prediction": "(b)",
        "true_answer": "(d)"
      },
      {
        "question": "Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date today in MM/DD/YYYY?\nOptions:\n(A) 08/28/2017\n(B) 02/22/2017\n(C) 03/01/2017\n(D) 01/28/2018\n(E) 02/07/2017\n(F) 02/28/2017",
        "prediction": "(f)",
        "true_answer": "(f)"
      },
      {
        "question": "Today, 8/3/1997, is a day that we will never forget. What is the date a month ago in MM/DD/YYYY?\nOptions:\n(A) 06/12/1997\n(B) 07/03/1997\n(C) 08/03/1996\n(D) 07/02/1997\n(E) 09/22/1997\n(F) 07/12/1997",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Today is Christmas Eve of 1937. What is the date one year ago from today in MM/DD/YYYY?\nOptions:\n(A) 12/24/1862\n(B) 01/10/1937\n(C) 10/24/1936\n(D) 12/24/1936\n(E) 03/24/1936\n(F) 12/29/1936",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "disambiguation_qa",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 70.0,
    "ft_time": 15.7782,
    "eval_time": 8.5945,
    "examples": [
      {
        "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The carpenter collaborated with the designer and gave them a prototype.\nOptions:\n(A) Gave the carpenter a prototype\n(B) Gave the designer a prototype\n(C) Ambiguous",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: My parent called her secretary for more information\nOptions:\n(A) They were my parent's secretary\n(B) They were the secretary's secretary\n(C) Ambiguous",
        "prediction": "(c)",
        "true_answer": "(a)"
      },
      {
        "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The patient disclosed to the counselor that he had a history of substance abuse.\nOptions:\n(A) The patient had a history\n(B) The counselor had a history\n(C) Ambiguous",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The sheriff protected the writer because she upholds the peace.\nOptions:\n(A) The sheriff upholds the peace\n(B) The writer upholds the peace\n(C) Ambiguous",
        "prediction": "(c)",
        "true_answer": "(a)"
      },
      {
        "question": "In the following sentences, explain the antecedent of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.\nSentence: The mechanic disliked the cashier because they are arrogant.\nOptions:\n(A) The mechanic is arrogant\n(B) The cashier is arrogant\n(C) Ambiguous",
        "prediction": "(c)",
        "true_answer": "(c)"
      }
    ]
  },
  {
    "task": "dyck_languages",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 67.9167,
    "ft_time": 13.7755,
    "eval_time": 4.4144,
    "examples": [
      {
        "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: { < [ < ( < [ ] > ) < > < { } > { < > { } { { < [ < < { } [ { } ] < > > > ] < > > } < [ [ ] ] > } } > ] [ ( ) ] > } [ { ( < ( ( ( [ { } ] ) ) ) > ) } ] [ < ( ) ( ( { ( ) [ { { ( { } ) } } ] } ) ) >",
        "prediction": ") ]",
        "true_answer": "]"
      },
      {
        "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: ( { ( ) { [ ] { ( ( { < > } ) ) ( < < > < > > < ( ( ) ) > ) } < ( < < < { } > > > ) > } }",
        "prediction": ")",
        "true_answer": ")"
      },
      {
        "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: { ( { { < { < > } > } < { } > < < < [ < [ ] > ] > > >",
        "prediction": "} ) }",
        "true_answer": "} ) }"
      },
      {
        "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: ( ( [ < [ ] > { { [ [ ] ] } } ]",
        "prediction": ") )",
        "true_answer": ") )"
      },
      {
        "question": "Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: < [ < > ]",
        "prediction": ">",
        "true_answer": ">"
      }
    ]
  },
  {
    "task": "formal_fallacies",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 60.4167,
    "ft_time": 17.498,
    "eval_time": 12.0772,
    "examples": [
      {
        "question": "\"Is Titanium oxide an ingredient of my washing power? Which chemicals does my perfume contain? It is really difficult to keep track of all chemicals one is regularly exposed to. The following argument seeks to clarify some such relations: Every ingredient of Hawaiian Punch is not an ingredient of Ice Queen Anyone? or not an ingredient of Exfoliating Balm. Everything that is an ingredient of Black Shadow is an ingredient of Hawaiian Punch, too. In consequence, every ingredient of Black Shadow is an ingredient of Exfoliating Balm and an ingredient of Ice Queen Anyone?.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
        "prediction": "invalid",
        "true_answer": "invalid"
      },
      {
        "question": "\"It is not always easy to grasp who is consuming which products. The following argument pertains to this question: First, being an infrequent user of Aveda shampoo is necessary for not being an owner of a Bio Ionic shampoo. Second, every loyal buyer of American Crew shampoo is a frequent consumer of Celtic Sea Salt soap and an owner of a Bio Ionic shampoo. Third, nobody is neither a frequent consumer of Celtic Sea Salt soap nor an infrequent user of Aveda shampoo. From this follows: Every loyal buyer of American Crew shampoo is an infrequent user of Aveda shampoo.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
        "prediction": "invalid",
        "true_answer": "invalid"
      },
      {
        "question": "\"It is not always easy to see who is related to whom -- and in which ways. The following argument pertains to this question: Being a stepbrother of Benjamin is necessary for being a grandson of Alan. Every half-brother of Melvin is a stepbrother of Benjamin. Everyone who is not a cousin of Solomon and not a stepbrother of Dominic is a grandson of Alan or a half-brother of Melvin. So, necessarily, someone who is not a stepbrother of Benjamin is a cousin of Solomon or a stepbrother of Dominic.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
        "prediction": "valid",
        "true_answer": "valid"
      },
      {
        "question": "\"Some football fans admire various clubs, others love only a single team. But who is a fan of whom precisely? The following argument pertains to this question: To begin with, every ex-fan of Villarreal CF is a supporter of Everton FC and a friend of SK Rapid Wien. Moreover, Kirk is not both: a supporter of Everton FC and a friend of SK Rapid Wien. So, necessarily, it is not the case that Kirk is an ex-fan of Villarreal CF.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
        "prediction": "valid",
        "true_answer": "valid"
      },
      {
        "question": "\"Here comes a perfectly valid argument: First of all, every loyal buyer of Tocca soap is an occasional purchaser of Bentley Organic soap. Next, being a loyal buyer of Tocca soap is sufficient for being a frequent consumer of L'Oreal shampoo. Plus,some regular user of Lever soap is not an occasional purchaser of Bentley Organic soap or not a frequent consumer of L'Oreal shampoo. We may conclude that not every regular user of Lever soap is a loyal buyer of Tocca soap.\"\nIs the argument, given the explicitly stated premises, deductively valid or invalid?\nOptions:\n- valid \n- invalid",
        "prediction": "valid",
        "true_answer": "valid"
      }
    ]
  },
  {
    "task": "geometric_shapes",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 59.1667,
    "ft_time": 18.0079,
    "eval_time": 12.804,
    "examples": [
      {
        "question": "This SVG path element <path d=\"M 81.33,89.86 L 65.20,41.90\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
        "prediction": "(e)",
        "true_answer": "(e)"
      },
      {
        "question": "This SVG path element <path d=\"M 68.00,36.00 L 61.00,80.00 L 50.00,90.00 L 34.00,90.00 L 41.00,75.00 L 14.00,78.00 L 68.00,36.00\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
        "prediction": "(f)",
        "true_answer": "(c)"
      },
      {
        "question": "This SVG path element <path d=\"M 53.87,61.90 L 52.73,49.16 A 12.80,12.80 207.60 0,1 63.98,54.06 L 53.87,61.90\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
        "prediction": "(i)",
        "true_answer": "(i)"
      },
      {
        "question": "This SVG path element <path d=\"M 59.43,52.76 L 75.49,27.45 L 54.92,4.40 M 54.92,4.40 L 23.70,7.77 L 15.15,42.15 L 34.51,57.44 L 59.43,52.76\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle",
        "prediction": "(g)",
        "true_answer": "(c)"
      },
      {
        "question": "This SVG path element <path d=\"M 17.67,4.06 A 9.79,9.79 243.89 1,0 26.29,21.65 A 9.79,9.79 243.89 1,0 17.67,4.06\"/> draws a\nOptions:\n(A) circle\n(B) heptagon\n(C) hexagon\n(D) kite\n(E) line\n(F) octagon\n(G) pentagon\n(H) rectangle\n(I) sector\n(J) triangle\n(K) ellipse",
        "prediction": "(k)",
        "true_answer": "(k)"
      }
    ]
  },
  {
    "task": "hyperbaton",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 79.5833,
    "ft_time": 14.0302,
    "eval_time": 4.0408,
    "examples": [
      {
        "question": "Which sentence has the correct adjective order:\nOptions:\n(A) circular Pakistani exercise steel mysterious black old surfboard\n(B) mysterious old circular black Pakistani steel exercise surfboard",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which sentence has the correct adjective order:\nOptions:\n(A) cardboard ridiculous old-fashioned cat\n(B) ridiculous old-fashioned cardboard cat",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which sentence has the correct adjective order:\nOptions:\n(A) ridiculous midsize old-fashioned pyramidal gray wool driving sock\n(B) old-fashioned wool midsize pyramidal gray ridiculous driving sock",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Which sentence has the correct adjective order:\nOptions:\n(A) Japanese silver square awful motorcycle\n(B) awful square Japanese silver motorcycle",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which sentence has the correct adjective order:\nOptions:\n(A) Iranian nice lead old-fashioned monkey\n(B) nice old-fashioned Iranian lead monkey",
        "prediction": "(b)",
        "true_answer": "(b)"
      }
    ]
  },
  {
    "task": "logical_deduction_five_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 40.8333,
    "ft_time": 19.8906,
    "eval_time": 15.5392,
    "examples": [
      {
        "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: mangoes, pears, cantaloupes, watermelons, and loquats. The mangoes are more expensive than the loquats. The cantaloupes are more expensive than the mangoes. The watermelons are more expensive than the cantaloupes. The pears are the cheapest.\nOptions:\n(A) The mangoes are the third-most expensive\n(B) The pears are the third-most expensive\n(C) The cantaloupes are the third-most expensive\n(D) The watermelons are the third-most expensive\n(E) The loquats are the third-most expensive",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were five golfers: Joe, Mya, Eve, Amy, and Ana. Eve finished below Joe. Ana finished below Amy. Eve finished above Amy. Mya finished above Joe.\nOptions:\n(A) Joe finished last\n(B) Mya finished last\n(C) Eve finished last\n(D) Amy finished last\n(E) Ana finished last",
        "prediction": "(a)",
        "true_answer": "(e)"
      },
      {
        "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells five fruits: apples, mangoes, oranges, plums, and pears. The oranges are more expensive than the mangoes. The plums are the cheapest. The apples are less expensive than the mangoes. The pears are more expensive than the oranges.\nOptions:\n(A) The apples are the second-cheapest\n(B) The mangoes are the second-cheapest\n(C) The oranges are the second-cheapest\n(D) The plums are the second-cheapest\n(E) The pears are the second-cheapest",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a truck, a station wagon, a motorcyle, a convertible, and a hatchback. The convertible is newer than the truck. The station wagon is newer than the hatchback. The convertible is older than the hatchback. The station wagon is the second-newest.\nOptions:\n(A) The truck is the third-newest\n(B) The station wagon is the third-newest\n(C) The motorcyle is the third-newest\n(D) The convertible is the third-newest\n(E) The hatchback is the third-newest",
        "prediction": "(a)",
        "true_answer": "(e)"
      },
      {
        "question": "The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are five vehicles: a hatchback, a limousine, a sedan, a bus, and a tractor. The tractor is newer than the hatchback. The bus is the second-oldest. The hatchback is newer than the sedan. The limousine is the newest.\nOptions:\n(A) The hatchback is the second-oldest\n(B) The limousine is the second-oldest\n(C) The sedan is the second-oldest\n(D) The bus is the second-oldest\n(E) The tractor is the second-oldest",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "logical_deduction_seven_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 48.3333,
    "ft_time": 23.1222,
    "eval_time": 20.6794,
    "examples": [
      {
        "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are seven vehicles: a bus, a motorcyle, a hatchback, a station wagon, a minivan, a truck, and a limousine. The station wagon is the fourth-newest. The motorcyle is newer than the truck. The station wagon is older than the hatchback. The minivan is newer than the hatchback. The bus is newer than the minivan. The truck is newer than the limousine.\nOptions:\n(A) The bus is the newest\n(B) The motorcyle is the newest\n(C) The hatchback is the newest\n(D) The station wagon is the newest\n(E) The minivan is the newest\n(F) The truck is the newest\n(G) The limousine is the newest",
        "prediction": "(g)",
        "true_answer": "(a)"
      },
      {
        "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. A fruit stand sells seven fruits: mangoes, watermelons, peaches, kiwis, oranges, cantaloupes, and plums. The watermelons are the cheapest. The peaches are more expensive than the mangoes. The cantaloupes are the second-most expensive. The oranges are more expensive than the cantaloupes. The peaches are less expensive than the plums. The kiwis are the third-cheapest.\nOptions:\n(A) The mangoes are the most expensive\n(B) The watermelons are the most expensive\n(C) The peaches are the most expensive\n(D) The kiwis are the most expensive\n(E) The oranges are the most expensive\n(F) The cantaloupes are the most expensive\n(G) The plums are the most expensive",
        "prediction": "(g)",
        "true_answer": "(e)"
      },
      {
        "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are seven books: a brown book, a gray book, an orange book, a green book, a black book, a purple book, and a yellow book. The purple book is the leftmost. The brown book is to the right of the yellow book. The green book is to the left of the orange book. The orange book is to the left of the gray book. The black book is the third from the left. The green book is the third from the right.\nOptions:\n(A) The brown book is the second from the left\n(B) The gray book is the second from the left\n(C) The orange book is the second from the left\n(D) The green book is the second from the left\n(E) The black book is the second from the left\n(F) The purple book is the second from the left\n(G) The yellow book is the second from the left",
        "prediction": "(c)",
        "true_answer": "(g)"
      },
      {
        "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. In a golf tournament, there were seven golfers: Eve, Mya, Rob, Ana, Ada, Mel, and Joe. Eve finished third. Mya finished above Joe. Mel finished above Ada. Mya finished above Rob. Mel finished below Joe. Mya finished second. Ada finished second-to-last.\nOptions:\n(A) Eve finished last\n(B) Mya finished last\n(C) Rob finished last\n(D) Ana finished last\n(E) Ada finished last\n(F) Mel finished last\n(G) Joe finished last",
        "prediction": "(e)",
        "true_answer": "(c)"
      },
      {
        "question": "The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are seven books: a red book, a white book, a purple book, a black book, a gray book, an orange book, and a blue book. The red book is to the right of the white book. The gray book is to the left of the orange book. The gray book is to the right of the blue book. The red book is the second from the right. The black book is the fourth from the left. The orange book is to the left of the white book.\nOptions:\n(A) The red book is the fourth from the left\n(B) The white book is the fourth from the left\n(C) The purple book is the fourth from the left\n(D) The black book is the fourth from the left\n(E) The gray book is the fourth from the left\n(F) The orange book is the fourth from the left\n(G) The blue book is the fourth from the left",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "logical_deduction_three_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 64.1667,
    "ft_time": 17.9888,
    "eval_time": 10.4164,
    "examples": [
      {
        "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a station wagon, a minivan, and a truck. The minivan is newer than the truck. The station wagon is newer than the minivan.\nOptions:\n(A) The station wagon is the oldest\n(B) The minivan is the oldest\n(C) The truck is the oldest",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a motorcyle, a limousine, and a convertible. The motorcyle is newer than the limousine. The convertible is newer than the motorcyle.\nOptions:\n(A) The motorcyle is the second-newest\n(B) The limousine is the second-newest\n(C) The convertible is the second-newest",
        "prediction": "(c)",
        "true_answer": "(a)"
      },
      {
        "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a sedan, a tractor, and a bus. The sedan is older than the tractor. The bus is older than the sedan.\nOptions:\n(A) The sedan is the newest\n(B) The tractor is the newest\n(C) The bus is the newest",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a convertible, a motorcyle, and a truck. The motorcyle is newer than the convertible. The convertible is newer than the truck.\nOptions:\n(A) The convertible is the second-newest\n(B) The motorcyle is the second-newest\n(C) The truck is the second-newest",
        "prediction": "(b)",
        "true_answer": "(a)"
      },
      {
        "question": "The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. In an antique car show, there are three vehicles: a motorcyle, a bus, and a tractor. The motorcyle is the oldest. The bus is newer than the tractor.\nOptions:\n(A) The motorcyle is the newest\n(B) The bus is the newest\n(C) The tractor is the newest",
        "prediction": "(c)",
        "true_answer": "(b)"
      }
    ]
  },
  {
    "task": "movie_recommendation",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 94.1667,
    "ft_time": 15.6465,
    "eval_time": 7.1293,
    "examples": [
      {
        "question": "Find a movie similar to The Shawshank Redemption, Forrest Gump, Dances with Wolves, The Fugitive:\nOptions:\n(A) Pulp Fiction\n(B) The Girl on the Bridge\n(C) Radioland Murders\n(D) Father of the Bride",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Find a movie similar to Batman, Forrest Gump, Jurassic Park, The Usual Suspects:\nOptions:\n(A) Apollo 13\n(B) Battle Beyond the Stars\n(C) Great Balls of Fire!\n(D) The Atomic Submarine",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Find a movie similar to Pulp Fiction, Forrest Gump, The Fugitive, Braveheart:\nOptions:\n(A) Little Women\n(B) Dances with Wolves\n(C) Bring It On All or Nothing\n(D) Dead Alive",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Find a movie similar to The Fifth Element, Gladiator, Gattaca, The Matrix:\nOptions:\n(A) Birdman of Alcatraz\n(B) Raiders of the Lost Ark\n(C) Le Samoura\u00ef\n(D) Imagine That",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Find a movie similar to Pulp Fiction, The Dark Knight, The Silence of the Lambs, Forrest Gump:\nOptions:\n(A) The Da Vinci Code\n(B) Inception\n(C) Identity Thief\n(D) War for the Planet of the Apes",
        "prediction": "(b)",
        "true_answer": "(b)"
      }
    ]
  },
  {
    "task": "multistep_arithmetic_two",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 1.25,
    "ft_time": 13.9286,
    "eval_time": 3.7044,
    "examples": [
      {
        "question": "((-3 - -6 - -7 + 3) + (5 * 5 * 1 - -3)) =",
        "prediction": "73",
        "true_answer": "41"
      },
      {
        "question": "((-7 * 2 + -4 * -8) - (-5 * 0 * 1 * 5)) =",
        "prediction": "84",
        "true_answer": "18"
      },
      {
        "question": "((0 * 8 - -9 * 6) - (-3 * -9 * -1 - 7)) =",
        "prediction": "198",
        "true_answer": "88"
      },
      {
        "question": "((-5 - -1 - 7 * 8) * (-1 - -4 * -7 * 2)) =",
        "prediction": "196",
        "true_answer": "3420"
      },
      {
        "question": "((-1 + -6 - -4 + -6) - (-2 * -5 - -1 * 1)) =",
        "prediction": "-20",
        "true_answer": "-20"
      }
    ]
  },
  {
    "task": "navigate",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 52.9167,
    "ft_time": 13.934,
    "eval_time": 5.9335,
    "examples": [
      {
        "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 9 steps left. Take 7 steps forward. Take 9 steps right. Take 7 steps left. Take 7 steps backward. Take 8 steps right. Take 10 steps left. Take 1 step backward. Take 1 step forward.\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "no"
      },
      {
        "question": "If you follow these instructions, do you return to the starting point? Take 1 step. Take 5 steps. Take 1 step. Turn left. Turn left. Take 9 steps.\nOptions:\n- Yes\n- No",
        "prediction": "no",
        "true_answer": "no"
      },
      {
        "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 10 steps forward. Take 4 steps backward. Take 6 steps forward.\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "no"
      },
      {
        "question": "If you follow these instructions, do you return to the starting point? Take 10 steps. Take 2 steps. Take 3 steps. Turn around. Take 4 steps. Turn around. Take 4 steps.\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "no"
      },
      {
        "question": "If you follow these instructions, do you return to the starting point? Always face forward. Take 5 steps left. Take 5 steps left. Take 10 steps right.\nOptions:\n- Yes\n- No",
        "prediction": "yes",
        "true_answer": "yes"
      }
    ]
  },
  {
    "task": "object_counting",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 60.0,
    "ft_time": 13.7544,
    "eval_time": 4.1168,
    "examples": [
      {
        "question": "I have a pig, a cow, three frogs, a donkey, a snake, a chicken, a cat, a snail, four mice, and three bears. How many animals do I have?",
        "prediction": "16",
        "true_answer": "17"
      },
      {
        "question": "I have a head of broccoli, and a yam. How many vegetables do I have?",
        "prediction": "2",
        "true_answer": "2"
      },
      {
        "question": "I have a trumpet, a flute, an accordion, and a trombone. How many musical instruments do I have?",
        "prediction": "4",
        "true_answer": "4"
      },
      {
        "question": "I have a trombone, a clarinet, and a drum. How many musical instruments do I have?",
        "prediction": "3",
        "true_answer": "3"
      },
      {
        "question": "I have two beds, a stove, a microwave, a table, a fridge, a lamp, and a toaster. How many objects do I have?",
        "prediction": "8",
        "true_answer": "8"
      }
    ]
  },
  {
    "task": "penguins_in_a_table",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 36.0294,
    "ft_time": 21.7633,
    "eval_time": 11.1572,
    "examples": [
      {
        "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  And here is a similar table, but listing giraffes:\nname, age, height (cm), weight (kg)\nJody, 5, 430, 620\nGladys, 10, 420, 590\nMarian, 2, 310, 410\nDonna, 9, 440, 650\nWhat is the name of the last penguin?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James",
        "prediction": "(d)",
        "true_answer": "(d)"
      },
      {
        "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  Which penguin is older than the other ones?\nOptions:\n(A) Louis\n(B) Bernard\n(C) Vincent\n(D) Gwen\n(E) James",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  What is the average height of the penguins?\nOptions:\n(A) 60\n(B) 65\n(C) 70\n(D) 75\n(E) 80",
        "prediction": "(c)",
        "true_answer": "(b)"
      },
      {
        "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We now add a penguin to the table:\nJames, 12, 90, 12\nWe then delete the penguin named Bernard from the table.\nHow many penguins are less than 8 years old and weight less than 12 kg?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5",
        "prediction": "(c)",
        "true_answer": "(a)"
      },
      {
        "question": "Here is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  How many penguins are more than 5 years old and weight more than 12 kg?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5",
        "prediction": "(c)",
        "true_answer": "(a)"
      }
    ]
  },
  {
    "task": "reasoning_about_colored_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 58.3333,
    "ft_time": 18.2777,
    "eval_time": 12.2327,
    "examples": [
      {
        "question": "On the desk, there are three turquoise plates, one pink plate, and three pink booklets. If I remove all the pink things from the desk, how many booklets remain on it?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six\n(H) seven\n(I) eight\n(J) nine\n(K) ten\n(L) eleven\n(M) twelve\n(N) thirteen\n(O) fourteen\n(P) fifteen\n(Q) sixteen",
        "prediction": "(b)",
        "true_answer": "(a)"
      },
      {
        "question": "On the desk, I see two silver booklets, one yellow sheet of paper, one silver pair of sunglasses, two yellow booklets, one yellow pair of sunglasses, three purple booklets, one purple sheet of paper, and two silver sheets of paper. If I remove all the silver items from the desk, how many pairs of sunglasses remain on it?\nOptions:\n(A) zero\n(B) one\n(C) two\n(D) three\n(E) four\n(F) five\n(G) six\n(H) seven\n(I) eight\n(J) nine\n(K) ten\n(L) eleven\n(M) twelve\n(N) thirteen\n(O) fourteen\n(P) fifteen\n(Q) sixteen",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "On the desk, you see a set of things arranged in a row: a yellow mug, a turquoise notebook, a black crayon, a brown pair of sunglasses, and a gold paperclip. What is the color of the thing directly to the left of the pair of sunglasses?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
        "prediction": "(o)",
        "true_answer": "(o)"
      },
      {
        "question": "On the table, there is a red dog leash, a brown teddy bear, a silver pencil, and a teal paperclip. What color is the paperclip?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
        "prediction": "(j)",
        "true_answer": "(j)"
      },
      {
        "question": "On the table, you see the following objects arranged in a row: a gold pencil, a red jug, a grey bracelet, and a mauve dog leash. What is the color of the object directly to the left of the bracelet?\nOptions:\n(A) red\n(B) orange\n(C) yellow\n(D) green\n(E) blue\n(F) brown\n(G) magenta\n(H) fuchsia\n(I) mauve\n(J) teal\n(K) turquoise\n(L) burgundy\n(M) silver\n(N) gold\n(O) black\n(P) grey\n(Q) purple\n(R) pink",
        "prediction": "(p)",
        "true_answer": "(a)"
      }
    ]
  },
  {
    "task": "ruin_names",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 85.8333,
    "ft_time": 14.7172,
    "eval_time": 6.0271,
    "examples": [
      {
        "question": "Which of the following is a humorous edit of this artist or movie name: 'fast and furious'?\nOptions:\n(A) fast andm furious\n(B) last and furious\n(C) fast yand furious\n(D) fast and furuous",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which of the following is a humorous edit of this artist or movie name: 'my fair lady'?\nOptions:\n(A) my fair laddy\n(B) my fmair lady\n(C) my faird lady\n(D) my fair laady",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Which of the following is a humorous edit of this artist or movie name: 'the men who stare at goats'?\nOptions:\n(A) the men who stare at boats\n(B) uhe men who stare at goats\n(C) the men who stare vt goats\n(D) the omen who stare at goats",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Which of the following is a humorous edit of this artist or movie name: 'the artist'?\nOptions:\n(A) thw artist\n(B) kthe artist\n(C) xthe artist\n(D) toe artist",
        "prediction": "(d)",
        "true_answer": "(d)"
      },
      {
        "question": "Which of the following is a humorous edit of this artist or movie name: 'ice cube'?\nOptions:\n(A) ice cunbe\n(B) icep cube\n(C) ice cmbe\n(D) lice cube",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "salient_translation_error_detection",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 53.3333,
    "ft_time": 25.7635,
    "eval_time": 26.3606,
    "examples": [
      {
        "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Der Gro\u00dfe Preis von S\u00fcdafrika 1962 fand am 29.\nTranslation: The 1962 South African Grand Prix was held on 29 September 1962.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
        "prediction": "(b)",
        "true_answer": "(f)"
      },
      {
        "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: In Mexiko gibt es 67 Nationalparks.\nTranslation: There are 77 national parks in Mexico.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Nimrod ist eine Kleinstadt im Wadena County des Bundesstaates Minnesota der Vereinigten Staaten.\nTranslation: Nimrod is a city in Wadena County, Minnesota.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
        "prediction": "(d)",
        "true_answer": "(a)"
      },
      {
        "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Als Hauptdolomit bezeichnet man eine bis zu 2200 Meter m\u00e4chtige lithostratigraphische Einheit der alpidischen Gebirge, die prim\u00e4r aus Dolomit aufgebaut ist.\nTranslation: The main dolomite is a lithostratigraphic unit of the Alpid mountains, which is up to 22000 metres in size, which is primarily made up of dolomite.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "The following translations from German to English contain a particular error. That error will be one of the following types: Named Entities: An entity (names, places, locations, etc.) is changed to a different entity. Numerical Values: Numerical values (ordinals or cardinals), dates, and/or units are changed. Modifiers or Adjectives: The modifiers and adjectives pertaining to a noun are changed. Negation or Antonyms: Introduce or remove a negation or change comparatives to their antonyms. Facts: Trivial factual errors not pertaining to the above classes are introduced in the translations. Dropped Content: A significant clause in the translation is removed. Please identify that error.  Source: Pedro Moren\u00e9s y \u00c1lvarez de Eulate ist ein spanischer Politiker der Partido Popular.\nTranslation: Pedro is a Spanish politician of the Popular Party.\nThe translation contains an error pertaining to\nOptions:\n(A) Modifiers or Adjectives\n(B) Numerical Values\n(C) Negation or Antonyms\n(D) Named Entities\n(E) Dropped Content\n(F) Facts",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "snarks",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 70.8333,
    "ft_time": 15.0898,
    "eval_time": 4.7845,
    "examples": [
      {
        "question": "Which statement is sarcastic?\nOptions:\n(A) Yeah, let's crucify this nonprofit for making money, they shouldn't be doing that\n(B) Yeah, let's crucify this company for making money, they shouldn't be doing that",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which statement is sarcastic?\nOptions:\n(A) Oh no the cleats with the new spikes didn't solve the problem?\n(B) Oh no the cleats with the new spikes didn't solve every problem?",
        "prediction": "(b)",
        "true_answer": "(b)"
      },
      {
        "question": "Which statement is sarcastic?\nOptions:\n(A) I love it when people mock other people for trying to get healthier. It's so classy\n(B) I love it when people encourage other people for trying to get healthier. It's so classy",
        "prediction": "(a)",
        "true_answer": "(a)"
      },
      {
        "question": "Which statement is sarcastic?\nOptions:\n(A) Yeah, pay-to-win gated content is much more fun and accessible to everyone\n(B) Yeah, pay-to-win gated content is much less fun and accessible to everyone",
        "prediction": "(b)",
        "true_answer": "(a)"
      },
      {
        "question": "Which statement is sarcastic?\nOptions:\n(A) Yeah, history is rarely black and white\n(B) Yeah, history is usually black and white",
        "prediction": "(b)",
        "true_answer": "(b)"
      }
    ]
  },
  {
    "task": "sports_understanding",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 80.4167,
    "ft_time": 12.9815,
    "eval_time": 2.7673,
    "examples": [
      {
        "question": "Is the following sentence plausible? \"Igor Shesterkin launched a hail mary.\"",
        "prediction": "yes",
        "true_answer": "no"
      },
      {
        "question": "Is the following sentence plausible? \"DeVante Parker was called for icing.\"",
        "prediction": "no",
        "true_answer": "no"
      },
      {
        "question": "Is the following sentence plausible? \"Didier Drogba got into the endzone.\"",
        "prediction": "no",
        "true_answer": "no"
      },
      {
        "question": "Is the following sentence plausible? \"Eden Hazard hit the buzzer beater.\"",
        "prediction": "no",
        "true_answer": "no"
      },
      {
        "question": "Is the following sentence plausible? \"Javier Zanetti watched the pitch go by in the National League Championship Series.\"",
        "prediction": "no",
        "true_answer": "no"
      }
    ]
  },
  {
    "task": "temporal_sequences",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 97.0833,
    "ft_time": 21.5648,
    "eval_time": 18.622,
    "examples": [
      {
        "question": "Today, James went to the physics classroom. Between what times could they have gone?\nWe know that:\nJames woke up at 8am.\nEmily saw James buying clothes at the mall from 8am to 9am.\nLisa saw James buying a phone at the electronics store from 9am to 4pm.\nSteven saw James driving to the water park from 5pm to 6pm.\nThe physics classroom was closed after 6pm.\nBetween what times could James have gone to the physics classroom?\nOptions:\n(A) 8am to 9am\n(B) 9am to 4pm\n(C) 5pm to 6pm\n(D) 4pm to 5pm",
        "prediction": "(d)",
        "true_answer": "(d)"
      },
      {
        "question": "Today, Sarah went to the soccer field. Between what times could they have gone?\nWe know that:\nSarah woke up at 5am.\nThomas saw Sarah sitting on a rooftop from 11am to 3pm.\nAndrew saw Sarah fixing their computer at the electronic store from 3pm to 5pm.\nEmily saw Sarah waiting at the airport from 5pm to 9pm.\nThe soccer field was closed after 9pm.\nBetween what times could Sarah have gone to the soccer field?\nOptions:\n(A) 3pm to 5pm\n(B) 11am to 3pm\n(C) 5am to 11am\n(D) 5pm to 9pm",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "Today, Kimberly went to the market. Between what times could they have gone?\nWe know that:\nKimberly woke up at 5am.\nNancy saw Kimberly getting a coffee at the cafe from 5am to 3pm.\nAnthony saw Kimberly taking photos near the Leaning Tower of Pisa from 3pm to 5pm.\nAndrew saw Kimberly buying a bike at the bike shop from 5pm to 6pm.\nSamantha saw Kimberly fixing their computer at the electronic store from 6pm to 7pm.\nThe market was closed after 9pm.\nBetween what times could Kimberly have gone to the market?\nOptions:\n(A) 3pm to 5pm\n(B) 5pm to 6pm\n(C) 7pm to 9pm\n(D) 6pm to 7pm",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "Today, Mary went to the amusement park. Between what times could they have gone?\nWe know that:\nMary woke up at 1pm.\nRichard saw Mary stretching at a yoga studio from 1pm to 3pm.\nJessica saw Mary taking photos near the Leaning Tower of Pisa from 3pm to 4pm.\nSamantha saw Mary fixing their computer at the electronic store from 4pm to 6pm.\nSean saw Mary getting a coffee at the cafe from 6pm to 7pm.\nKimberly saw Mary working at the office from 9pm to 10pm.\nThe amusement park was closed after 10pm.\nBetween what times could Mary have gone to the amusement park?\nOptions:\n(A) 9pm to 10pm\n(B) 1pm to 3pm\n(C) 3pm to 4pm\n(D) 7pm to 9pm",
        "prediction": "(d)",
        "true_answer": "(d)"
      },
      {
        "question": "Today, Richard went to the bakery. Between what times could they have gone?\nWe know that:\nRichard woke up at 8am.\nBetty saw Richard reading at the library from 9am to 2pm.\nDavid saw Richard fixing their computer at the electronic store from 2pm to 4pm.\nMary saw Richard taking photos near the Eiffel Tower from 4pm to 5pm.\nHannah saw Richard sitting on a rooftop from 5pm to 6pm.\nSusan saw Richard driving to the water park from 6pm to 7pm.\nThe bakery was closed after 7pm.\nBetween what times could Richard have gone to the bakery?\nOptions:\n(A) 6pm to 7pm\n(B) 9am to 2pm\n(C) 5pm to 6pm\n(D) 8am to 9am",
        "prediction": "(d)",
        "true_answer": "(d)"
      }
    ]
  },
  {
    "task": "tracking_shuffled_objects_five_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 29.1667,
    "ft_time": 21.6777,
    "eval_time": 17.6939,
    "examples": [
      {
        "question": "Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Sam, Bob is dancing with Izzi, Claire is dancing with Karl, Dave is dancing with Helga, and Eve is dancing with Ophelia.\nThroughout the song, the dancers often trade partners. First, Bob and Alice switch partners. Then, Alice and Claire switch partners. Then, Alice and Dave switch partners. Then, Alice and Eve switch partners. Finally, Dave and Claire switch partners. At the end of the dance, Claire is dancing with\nOptions:\n(A) Sam\n(B) Izzi\n(C) Karl\n(D) Helga\n(E) Ophelia",
        "prediction": "(d)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a blue present, Bob has a black ball, Claire has a brown present, Dave has a orange ball, and Eve has a red present.\nAs the event progresses, pairs of people swap gifts. First, Bob and Dave swap their gifts. Then, Alice and Dave swap their gifts. Then, Eve and Claire swap their gifts. Then, Alice and Bob swap their gifts. Finally, Dave and Claire swap their gifts. At the end of the event, Alice has the\nOptions:\n(A) blue present\n(B) black ball\n(C) brown present\n(D) orange ball\n(E) red present",
        "prediction": "(d)",
        "true_answer": "(d)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, and Eve are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Karl, Bob is dancing with Melissa, Claire is dancing with Lola, Dave is dancing with Sam, and Eve is dancing with Izzi.\nThroughout the song, the dancers often trade partners. First, Claire and Dave switch partners. Then, Eve and Bob switch partners. Then, Claire and Alice switch partners. Then, Eve and Alice switch partners. Finally, Claire and Dave switch partners. At the end of the dance, Alice is dancing with\nOptions:\n(A) Karl\n(B) Melissa\n(C) Lola\n(D) Sam\n(E) Izzi",
        "prediction": "(e)",
        "true_answer": "(b)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, and Eve are playing a game. At the start of the game, they are each holding a ball: Alice has a black ball, Bob has a purple ball, Claire has a green ball, Dave has a brown ball, and Eve has a white ball.\nAs the game progresses, pairs of players trade balls. First, Eve and Bob swap balls. Then, Dave and Claire swap balls. Then, Dave and Alice swap balls. Then, Dave and Claire swap balls. Finally, Alice and Claire swap balls. At the end of the game, Bob has the\nOptions:\n(A) black ball\n(B) purple ball\n(C) green ball\n(D) brown ball\n(E) white ball",
        "prediction": "(b)",
        "true_answer": "(e)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, and Eve are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a brown present, Bob has a black ball, Claire has a green present, Dave has a purple present, and Eve has a yellow present.\nAs the event progresses, pairs of people swap gifts. First, Bob and Dave swap their gifts. Then, Bob and Eve swap their gifts. Then, Dave and Claire swap their gifts. Then, Dave and Bob swap their gifts. Finally, Alice and Dave swap their gifts. At the end of the event, Dave has the\nOptions:\n(A) brown present\n(B) black ball\n(C) green present\n(D) purple present\n(E) yellow present",
        "prediction": "(a)",
        "true_answer": "(a)"
      }
    ]
  },
  {
    "task": "tracking_shuffled_objects_seven_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 13.3333,
    "ft_time": 24.9935,
    "eval_time": 23.6699,
    "examples": [
      {
        "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Karl, Bob is dancing with Izzi, Claire is dancing with Melissa, Dave is dancing with Lola, Eve is dancing with Rodrigo, Fred is dancing with Helga, and Gertrude is dancing with Patrick.\nThroughout the song, the dancers often trade partners. First, Alice and Bob switch partners. Then, Eve and Gertrude switch partners. Then, Claire and Gertrude switch partners. Then, Gertrude and Bob switch partners. Then, Claire and Bob switch partners. Then, Fred and Gertrude switch partners. Finally, Claire and Dave switch partners. At the end of the dance, Bob is dancing with\nOptions:\n(A) Karl\n(B) Izzi\n(C) Melissa\n(D) Lola\n(E) Rodrigo\n(F) Helga\n(G) Patrick",
        "prediction": "(a)",
        "true_answer": "(e)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a blue present, Bob has a green present, Claire has a brown present, Dave has a purple present, Eve has a orange ball, Fred has a pink ball, and Gertrude has a red present.\nAs the event progresses, pairs of people swap gifts. First, Eve and Claire swap their gifts. Then, Fred and Claire swap their gifts. Then, Dave and Bob swap their gifts. Then, Alice and Claire swap their gifts. Then, Dave and Alice swap their gifts. Then, Gertrude and Eve swap their gifts. Finally, Claire and Gertrude swap their gifts. At the end of the event, Claire has the\nOptions:\n(A) blue present\n(B) green present\n(C) brown present\n(D) purple present\n(E) orange ball\n(F) pink ball\n(G) red present",
        "prediction": "(a)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are dancers at a square dance. At the start of a song, they each have a partner: Alice is dancing with Lola, Bob is dancing with Ophelia, Claire is dancing with Sam, Dave is dancing with Helga, Eve is dancing with Patrick, Fred is dancing with Jamie, and Gertrude is dancing with Rodrigo.\nThroughout the song, the dancers often trade partners. First, Fred and Bob switch partners. Then, Eve and Alice switch partners. Then, Fred and Eve switch partners. Then, Claire and Fred switch partners. Then, Gertrude and Alice switch partners. Then, Alice and Bob switch partners. Finally, Dave and Fred switch partners. At the end of the dance, Dave is dancing with\nOptions:\n(A) Lola\n(B) Ophelia\n(C) Sam\n(D) Helga\n(E) Patrick\n(F) Jamie\n(G) Rodrigo",
        "prediction": "(a)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Catch-22, Bob gets Frankenstein, Claire gets Ulysses, Dave gets Hound of the Baskervilles, Eve gets Moby Dick, Fred gets The Fellowship of the Ring, and Gertrude gets The Odyssey.\nAs the semester proceeds, they start trading around the new books. First, Alice and Bob swap books. Then, Gertrude and Claire swap books. Then, Dave and Gertrude swap books. Then, Alice and Fred swap books. Then, Dave and Eve swap books. Then, Claire and Bob swap books. Finally, Eve and Bob swap books. At the end of the semester, Claire has\nOptions:\n(A) Catch-22\n(B) Frankenstein\n(C) Ulysses\n(D) Hound of the Baskervilles\n(E) Moby Dick\n(F) The Fellowship of the Ring\n(G) The Odyssey",
        "prediction": "(b)",
        "true_answer": "(a)"
      },
      {
        "question": "Alice, Bob, Claire, Dave, Eve, Fred, and Gertrude are friends and avid readers who occasionally trade books. At the start of the semester, they each buy one new book: Alice gets Moby Dick, Bob gets Hound of the Baskervilles, Claire gets The Great Gatsby, Dave gets The Odyssey, Eve gets Ulysses, Fred gets Lolita, and Gertrude gets The Pearl.\nAs the semester proceeds, they start trading around the new books. First, Alice and Fred swap books. Then, Eve and Fred swap books. Then, Fred and Bob swap books. Then, Eve and Gertrude swap books. Then, Gertrude and Claire swap books. Then, Claire and Bob swap books. Finally, Dave and Alice swap books. At the end of the semester, Bob has\nOptions:\n(A) Moby Dick\n(B) Hound of the Baskervilles\n(C) The Great Gatsby\n(D) The Odyssey\n(E) Ulysses\n(F) Lolita\n(G) The Pearl",
        "prediction": "(f)",
        "true_answer": "(a)"
      }
    ]
  },
  {
    "task": "tracking_shuffled_objects_three_objects",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 36.25,
    "ft_time": 18.0436,
    "eval_time": 12.6065,
    "examples": [
      {
        "question": "Alice, Bob, and Claire are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a pink ball, Bob has a black ball, and Claire has a purple present.\nAs the event progresses, pairs of people swap gifts. First, Bob and Claire swap their gifts. Then, Alice and Claire swap their gifts. Finally, Claire and Bob swap their gifts. At the end of the event, Alice has the\nOptions:\n(A) pink ball\n(B) black ball\n(C) purple present",
        "prediction": "(a)",
        "true_answer": "(b)"
      },
      {
        "question": "Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing fullback, Bob is playing cheerleader, and Claire is playing left winger.\nAs the game progresses, pairs of players occasionally swap positions. First, Alice and Claire trade positions. Then, Bob and Alice trade positions. Finally, Bob and Claire trade positions. At the end of the match, Claire is playing\nOptions:\n(A) fullback\n(B) cheerleader\n(C) left winger",
        "prediction": "(b)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing cheerleader, Bob is playing right winger, and Claire is playing fullback.\nAs the game progresses, pairs of players occasionally swap positions. First, Claire and Alice trade positions. Then, Bob and Claire trade positions. Finally, Claire and Alice trade positions. At the end of the match, Claire is playing\nOptions:\n(A) cheerleader\n(B) right winger\n(C) fullback",
        "prediction": "(b)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, and Claire are holding a white elephant gift exchange. At the start of the event, they are each holding a present of a different color: Alice has a pink ball, Bob has a blue present, and Claire has a brown present.\nAs the event progresses, pairs of people swap gifts. First, Claire and Alice swap their gifts. Then, Bob and Alice swap their gifts. Finally, Alice and Claire swap their gifts. At the end of the event, Bob has the\nOptions:\n(A) pink ball\n(B) blue present\n(C) brown present",
        "prediction": "(c)",
        "true_answer": "(c)"
      },
      {
        "question": "Alice, Bob, and Claire are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing left midfielder, Bob is playing right midfielder, and Claire is playing goalkeeper.\nAs the game progresses, pairs of players occasionally swap positions. First, Bob and Alice trade positions. Then, Alice and Claire trade positions. Finally, Bob and Alice trade positions. At the end of the match, Bob is playing\nOptions:\n(A) left midfielder\n(B) right midfielder\n(C) goalkeeper",
        "prediction": "(c)",
        "true_answer": "(c)"
      }
    ]
  },
  {
    "task": "web_of_lies",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 51.25,
    "ft_time": 14.6157,
    "eval_time": 5.494,
    "examples": [
      {
        "question": "Question: Osvaldo tells the truth. Andree says Osvaldo tells the truth. Willian says Andree lies. Vina says Willian tells the truth. Jaymie says Vina tells the truth. Does Jaymie tell the truth?",
        "prediction": "no",
        "true_answer": "no"
      },
      {
        "question": "Question: Willian tells the truth. Michaela says Willian tells the truth. Sima says Michaela lies. Inga says Sima tells the truth. Audrie says Inga lies. Does Audrie tell the truth?",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "Question: Phoebe tells the truth. Jamey says Phoebe lies. Audrie says Jamey tells the truth. Michaela says Audrie lies. Vina says Michaela lies. Does Vina tell the truth?",
        "prediction": "yes",
        "true_answer": "no"
      },
      {
        "question": "Question: Lorine lies. Osvaldo says Lorine lies. Alejandro says Osvaldo lies. Gwenn says Alejandro tells the truth. Dallas says Gwenn lies. Does Dallas tell the truth?",
        "prediction": "yes",
        "true_answer": "yes"
      },
      {
        "question": "Question: Ka tells the truth. Fletcher says Ka tells the truth. Maybelle says Fletcher lies. Lorine says Maybelle lies. Crista says Lorine tells the truth. Does Crista tell the truth?",
        "prediction": "no",
        "true_answer": "yes"
      }
    ]
  },
  {
    "task": "word_sorting",
    "ICFT_main_10_masked_inputs_text_completion_dataset_40_True_False_5_False_5_1_1e-4_64_64_0.05_5_accuracy": 58.75,
    "ft_time": 15.1751,
    "eval_time": 7.919,
    "examples": [
      {
        "question": "Sort the following words alphabetically: List: murk pvc singlet yeats muslim coachmen clergymen companion gazpacho embark pristine granular hobble fatten allot too niggle chauncey threefold coddington",
        "prediction": "allot coddington coachmen clergymen companion chauncey embark fatten gazpacho granular hobble murk muslim niggle pvc pristine singlet threefold too yeats",
        "true_answer": "allot chauncey clergymen coachmen coddington companion embark fatten gazpacho granular hobble murk muslim niggle pristine pvc singlet threefold too yeats"
      },
      {
        "question": "Sort the following words alphabetically: List: greasy lapidary mark",
        "prediction": "greasy lapidary mark",
        "true_answer": "greasy lapidary mark"
      },
      {
        "question": "Sort the following words alphabetically: List: berniece bremsstrahlung hoe avalanche showroom goff dactylic befriend hurry housekeep lanka gilbertson posterior flick bong hereafter metazoan",
        "prediction": "avalanche bong bremsstrahlung befriend berniece bong dactylic flick gilbertson goff hereafter housekeep hurry lanka metazoan posterior showroom",
        "true_answer": "avalanche befriend berniece bong bremsstrahlung dactylic flick gilbertson goff hereafter hoe housekeep hurry lanka metazoan posterior showroom"
      },
      {
        "question": "Sort the following words alphabetically: List: liggett sterno pintail dilate frolic piggish forth parameter slaughterhouse lacunae mcintosh bloodstain lazy unesco protector beachhead algonquin",
        "prediction": "algonquin beachhead bloodstain dilate frolic lacunae lazy liggett mcintosh parameter pintail protector sterno slaughterhouse unesco",
        "true_answer": "algonquin beachhead bloodstain dilate forth frolic lacunae lazy liggett mcintosh parameter piggish pintail protector slaughterhouse sterno unesco"
      },
      {
        "question": "Sort the following words alphabetically: List: tipoff fructose coleus hurricane cite tularemia irreducible wier improbable whitetail vesper whereabout",
        "prediction": "cite coleus fructose hurricane irreducible improbable tipoff tularemia vesper whereabout whitetail wier",
        "true_answer": "cite coleus fructose hurricane improbable irreducible tipoff tularemia vesper whereabout whitetail wier"
      }
    ]
  }
]